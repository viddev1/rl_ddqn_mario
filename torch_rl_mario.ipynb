{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZjb8ZzSRAkhD8GIYfygPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viddev1/rl_ddqn_mario/blob/main/torch_rl_mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6nTX3XySTjE",
        "outputId": "54f35639-611a-4c58-8762-51fa64a6e040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 KB 6.0 MB/s eta 0:00:00\n",
            "Collecting nes-py>=8.1.4\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 KB 3.2 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.21.6)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 25.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.64.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.12.1)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py): started\n",
            "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp38-cp38-linux_x86_64.whl size=498457 sha256=e437c2f77dc7b3833dfc15fb33e04cb4e09b2d5f3df54f1eac3d09b74a7fbf92\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/e5/5c/8dfae61b44dbf56c458483aa09accef55a650e0527f6cbd872\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "import gym_super_mario_bros\n"
      ],
      "metadata": {
        "id": "KSx_MIH0Sf_X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if gym.__version__ < '0.26':\n",
        "  env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "  env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = 'rgb', \n",
        "                                  apply_api_compatibility=True)\n",
        "\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape}, \\n {reward}, \\n {done},\\n {info}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAbRdkjUTPW1",
        "outputId": "57768850-ec3a-4956-ff82-ab8dedee00b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3), \n",
            " 0.0, \n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "  def __init__(self, env, skip):\n",
        "    super().__init__(env)\n",
        "    self._skip = skip\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0.0\n",
        "    for i in range(self._skip):\n",
        "      obs, reward, done, trunc, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return obs, total_reward, done, trunc, info\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    obs_shape = self.observation_space.shape[:2]\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def permute_orientation(self, observation):\n",
        "    observation = np.transpose(observation, (2, 0, 1))\n",
        "    observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "    return observation\n",
        "\n",
        "  def observation(self, observation):\n",
        "    observation = self.permute_orientation(observation)\n",
        "    transform = T.Grayscale()\n",
        "    observation = transform(observation)\n",
        "    return observation\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env, shape):\n",
        "    super().__init__(env)\n",
        "    if isinstance(shape, int):\n",
        "      self.shape = (shape, shape)\n",
        "    else:\n",
        "      self.shape = tuple(shape)\n",
        "    \n",
        "    obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def observation(self, observation):\n",
        "    transforms = T.Compose(\n",
        "        [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "    )\n",
        "    observation = transforms(observation).squeeze(0)\n",
        "    return observation\n",
        "\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "  env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "  env = FrameStack(env, num_stack=4)"
      ],
      "metadata": {
        "id": "wWwDKCT-UgWm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "  def __init__():\n",
        "    pass\n",
        "  \n",
        "  def act(self, state):\n",
        "    \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
        "    pass\n",
        "\n",
        "  def cache(self, experience):\n",
        "    \"\"\"Add the experience to memory\"\"\"\n",
        "    pass\n",
        "\n",
        "  def recall(self):\n",
        "    \"\"\"Sample experiences from memory\"\"\"\n",
        "    pass\n",
        "\n",
        "  def learn(self):\n",
        "    \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "4K5JEyMBZeSV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.save_dir = save_dir\n",
        "\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "    self.net = self.net.to(device=self.device)\n",
        "\n",
        "    self.exploration_rate = 1\n",
        "    self.exploration_rate_decay = 0.99999975\n",
        "    self.exploration_rate_min = 0.1\n",
        "    self.curr_step = 0\n",
        "\n",
        "    self.save_every = 5e5\n",
        "\n",
        "  def act(self, state):\n",
        "    \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    action_idx (int): An integer representing which action Mario will perform\n",
        "    \"\"\"\n",
        "\n",
        "    #Explore\n",
        "    if np.random.rand() < self.exploration_rate:\n",
        "      action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "    #Exploit\n",
        "    else:\n",
        "      state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "      state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "      action_values = self.net(state, model=\"online\")\n",
        "      action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "    #Decrease exploration_rate\n",
        "    self.exploration_rate *= self.exploration_rate_decay\n",
        "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "    #increment step\n",
        "    self.curr_step += 1\n",
        "    return action_idx\n",
        "\n"
      ],
      "metadata": {
        "id": "nqfhT3mFaw6y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.memory = deque(maxlen=100000)\n",
        "    self.batch_size = 32\n",
        "\n",
        "  def cache(self, state, next_state, action, reward, done):\n",
        "    \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (LazyFrame),\n",
        "        next_state (LazyFrame),\n",
        "        action (int),\n",
        "        reward (float),\n",
        "        done(bool))\n",
        "        \"\"\"\n",
        "    def first_if_tuple(x):\n",
        "      return x[0] if isinstance(x, tuple) else x\n",
        "    state = first_if_tuple(state).__array__()\n",
        "    next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "    state = torch.tensor(state, device= self.device)\n",
        "    next_state = torch.tensor(next_state, device=self.device)\n",
        "    action = torch.tensor([action], device=self.device)\n",
        "    reward = torch.tensor([reward], device=self.device)\n",
        "    done = torch.tensor([done], device=self.device)\n",
        "\n",
        "    self.memory.append((state, next_state, action, reward, done,))\n",
        "  \n",
        "  def recall(self):\n",
        "    \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "\n",
        "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "    "
      ],
      "metadata": {
        "id": "3fPp0O4adoJJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarioNet(nn.Module):\n",
        "  \"\"\"mini cnn structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    c, h, w = input_dim\n",
        "\n",
        "    if h != 84:\n",
        "      raise ValueError(f\"Expenting input height: 84, got: {h}\")\n",
        "    if w != 84:\n",
        "      raise ValueError(f\"Expenting input width: 84, got: {w}\")\n",
        "\n",
        "    self.online = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, output_dim)\n",
        "    )\n",
        "\n",
        "    self.target = copy.deepcopy(self.online)\n",
        "\n",
        "    # Q_target parameters are frozen.\n",
        "    for p in self.target.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "  def forward(self, input, model):\n",
        "    if model == \"online\":\n",
        "      return self.online(input)\n",
        "    elif model == \"target\":\n",
        "      return self.target(input)"
      ],
      "metadata": {
        "id": "z2oS1W2Ff4-5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.gamma = 0.9\n",
        "  \n",
        "  def td_estimate(self, state, action):\n",
        "    current_Q = self.net(state, model=\"online\")[\n",
        "        np.arange(0, self.batch_size), action\n",
        "    ]\n",
        "    return current_Q\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def td_target(self, reward, next_state, done):\n",
        "    next_state_Q = self.net(next_state, model=\"online\")\n",
        "    best_action = torch.argmax(next_state_Q, axis=1)\n",
        "    next_Q = self.net(next_state, model=\"target\")[\n",
        "        np.arange(0, self.batch_size), best_action\n",
        "    ]\n",
        "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ],
      "metadata": {
        "id": "5haEV_tGjaTz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "  def update_Q_online(self, td_estimate, td_target):\n",
        "    loss = self.loss_fn(td_estimate, td_target)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "  def sync_Q_target(self):\n",
        "    self.net.target.load_state_dict(self.net.online.state_dict())"
      ],
      "metadata": {
        "id": "gPhRSTDGlTDz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def save(self):\n",
        "    save_path = (\n",
        "        self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "    )\n",
        "    torch.save(\n",
        "        dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "        save_path\n",
        "    )\n",
        "    print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ],
      "metadata": {
        "id": "kuQXrt5jmb-S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.burnin = 1e4 # min. experiences before training\n",
        "    self.learn_every = 3 # no. of experiences between updates to Q_online\n",
        "    self.sync_every = 1e4 # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "  def learn(self):\n",
        "    if self.curr_step % self.sync_every == 0:\n",
        "      self.sync_Q_target()\n",
        "    \n",
        "    if self.curr_step % self.save_every == 0:\n",
        "      self.save()\n",
        "\n",
        "    if self.curr_step < self.burnin:\n",
        "      return None, None\n",
        "    \n",
        "    if self.curr_step % self.learn_every !=0:\n",
        "      return None, None\n",
        "    \n",
        "    # Sample from memory\n",
        "    state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "    # Get TD Estimate\n",
        "    td_est = self.td_estimate(state, action)\n",
        "\n",
        "    # Get TD Target\n",
        "    td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "    # Backpropagate loss through Q_online\n",
        "    loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "    return (td_est.mean().item(), loss)"
      ],
      "metadata": {
        "id": "Mejqtah0nERT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "metadata": {
        "id": "2e2HJDg-oVAr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 1000\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 10 == 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
        "    \n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-h0VE506ovEB",
        "outputId": "459d4c4f-2f07-4cfc-aefc-810aa1189512"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA: True\n",
            "\n",
            "Episode 0 - Step 98 - Epsilon 0.9999755002970567 - Mean Reward 626.0 - Mean Length 98.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 2.072 - Time 2023-02-17T00:41:39\n",
            "Episode 10 - Step 1668 - Epsilon 0.9995830868802537 - Mean Reward 605.545 - Mean Length 151.636 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 25.928 - Time 2023-02-17T00:42:05\n",
            "Episode 20 - Step 2902 - Epsilon 0.9992747630207686 - Mean Reward 569.381 - Mean Length 138.19 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 20.611 - Time 2023-02-17T00:42:25\n",
            "Episode 30 - Step 5074 - Epsilon 0.9987323040475058 - Mean Reward 610.677 - Mean Length 163.677 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 23.443 - Time 2023-02-17T00:42:49\n",
            "Episode 40 - Step 7152 - Epsilon 0.9982135972963848 - Mean Reward 624.78 - Mean Length 174.439 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 22.11 - Time 2023-02-17T00:43:11\n",
            "Episode 50 - Step 8590 - Epsilon 0.9978548039602222 - Mean Reward 622.0 - Mean Length 168.431 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.483 - Time 2023-02-17T00:43:26\n",
            "Episode 60 - Step 10676 - Epsilon 0.9973345582807055 - Mean Reward 619.541 - Mean Length 175.016 - Mean Loss 0.13 - Mean Q Value 0.215 - Time Delta 24.391 - Time 2023-02-17T00:43:50\n",
            "Episode 70 - Step 11842 - Epsilon 0.9970438775893604 - Mean Reward 603.254 - Mean Length 166.789 - Mean Loss 0.23 - Mean Q Value 0.722 - Time Delta 14.871 - Time 2023-02-17T00:44:05\n",
            "Episode 80 - Step 13965 - Epsilon 0.9965148368917196 - Mean Reward 608.259 - Mean Length 172.407 - Mean Loss 0.273 - Mean Q Value 1.087 - Time Delta 26.824 - Time 2023-02-17T00:44:32\n",
            "Episode 90 - Step 15746 - Epsilon 0.9960712373687537 - Mean Reward 626.56 - Mean Length 173.033 - Mean Loss 0.301 - Mean Q Value 1.373 - Time Delta 22.484 - Time 2023-02-17T00:44:54\n",
            "Episode 100 - Step 17312 - Epsilon 0.9956813517556224 - Mean Reward 627.69 - Mean Length 172.14 - Mean Loss 0.322 - Mean Q Value 1.625 - Time Delta 19.909 - Time 2023-02-17T00:45:14\n",
            "Episode 110 - Step 19759 - Epsilon 0.9950724298854522 - Mean Reward 635.32 - Mean Length 180.91 - Mean Loss 0.373 - Mean Q Value 1.989 - Time Delta 30.308 - Time 2023-02-17T00:45:44\n",
            "Episode 120 - Step 21628 - Epsilon 0.9946075908408978 - Mean Reward 640.65 - Mean Length 187.26 - Mean Loss 0.43 - Mean Q Value 2.664 - Time Delta 24.465 - Time 2023-02-17T00:46:09\n",
            "Episode 130 - Step 24042 - Epsilon 0.994007526172458 - Mean Reward 649.13 - Mean Length 189.68 - Mean Loss 0.489 - Mean Q Value 3.349 - Time Delta 29.839 - Time 2023-02-17T00:46:39\n",
            "Episode 140 - Step 26053 - Epsilon 0.9935079144265797 - Mean Reward 654.91 - Mean Length 189.01 - Mean Loss 0.544 - Mean Q Value 4.034 - Time Delta 25.366 - Time 2023-02-17T00:47:04\n",
            "Episode 150 - Step 28015 - Epsilon 0.9930207182282215 - Mean Reward 663.73 - Mean Length 194.25 - Mean Loss 0.594 - Mean Q Value 4.742 - Time Delta 24.369 - Time 2023-02-17T00:47:28\n",
            "Episode 160 - Step 30832 - Epsilon 0.9923216294954409 - Mean Reward 684.14 - Mean Length 201.56 - Mean Loss 0.569 - Mean Q Value 5.406 - Time Delta 35.521 - Time 2023-02-17T00:48:04\n",
            "Episode 170 - Step 32530 - Epsilon 0.99190047830668 - Mean Reward 693.79 - Mean Length 206.88 - Mean Loss 0.544 - Mean Q Value 5.972 - Time Delta 21.658 - Time 2023-02-17T00:48:26\n",
            "Episode 180 - Step 34837 - Epsilon 0.9913285645757227 - Mean Reward 698.22 - Mean Length 208.72 - Mean Loss 0.54 - Mean Q Value 6.537 - Time Delta 28.794 - Time 2023-02-17T00:48:54\n",
            "Episode 190 - Step 36836 - Epsilon 0.990833271835172 - Mean Reward 694.54 - Mean Length 210.9 - Mean Loss 0.542 - Mean Q Value 7.108 - Time Delta 25.181 - Time 2023-02-17T00:49:20\n",
            "Episode 200 - Step 39125 - Epsilon 0.9902664296276253 - Mean Reward 712.4 - Mean Length 218.13 - Mean Loss 0.547 - Mean Q Value 7.662 - Time Delta 30.173 - Time 2023-02-17T00:49:50\n",
            "Episode 210 - Step 42465 - Epsilon 0.9894399021786872 - Mean Reward 723.14 - Mean Length 227.06 - Mean Loss 0.56 - Mean Q Value 8.399 - Time Delta 41.507 - Time 2023-02-17T00:50:31\n",
            "Episode 220 - Step 45647 - Epsilon 0.9886531156238861 - Mean Reward 735.05 - Mean Length 240.19 - Mean Loss 0.563 - Mean Q Value 8.858 - Time Delta 39.649 - Time 2023-02-17T00:51:11\n",
            "Episode 230 - Step 47760 - Epsilon 0.9881309974668206 - Mean Reward 728.51 - Mean Length 237.18 - Mean Loss 0.56 - Mean Q Value 9.311 - Time Delta 29.459 - Time 2023-02-17T00:51:40\n",
            "Episode 240 - Step 49781 - Epsilon 0.9876318703205049 - Mean Reward 721.34 - Mean Length 237.28 - Mean Loss 0.562 - Mean Q Value 9.743 - Time Delta 26.364 - Time 2023-02-17T00:52:07\n",
            "Episode 250 - Step 51697 - Epsilon 0.9871589078789834 - Mean Reward 712.22 - Mean Length 236.82 - Mean Loss 0.581 - Mean Q Value 10.361 - Time Delta 24.196 - Time 2023-02-17T00:52:31\n",
            "Episode 260 - Step 53477 - Epsilon 0.9867197198366036 - Mean Reward 687.41 - Mean Length 226.45 - Mean Loss 0.602 - Mean Q Value 10.91 - Time Delta 22.758 - Time 2023-02-17T00:52:54\n",
            "Episode 270 - Step 55241 - Epsilon 0.9862846723207414 - Mean Reward 687.97 - Mean Length 227.11 - Mean Loss 0.604 - Mean Q Value 11.315 - Time Delta 22.689 - Time 2023-02-17T00:53:16\n",
            "Episode 280 - Step 56364 - Epsilon 0.9860078117304649 - Mean Reward 667.54 - Mean Length 215.27 - Mean Loss 0.618 - Mean Q Value 11.735 - Time Delta 14.032 - Time 2023-02-17T00:53:30\n",
            "Episode 290 - Step 58348 - Epsilon 0.9855188730614738 - Mean Reward 661.65 - Mean Length 215.12 - Mean Loss 0.623 - Mean Q Value 12.158 - Time Delta 24.956 - Time 2023-02-17T00:53:55\n",
            "Episode 300 - Step 59874 - Epsilon 0.9851429692725674 - Mean Reward 629.28 - Mean Length 207.49 - Mean Loss 0.626 - Mean Q Value 12.596 - Time Delta 19.636 - Time 2023-02-17T00:54:15\n",
            "Episode 310 - Step 60980 - Epsilon 0.9848706148622138 - Mean Reward 602.32 - Mean Length 185.15 - Mean Loss 0.646 - Mean Q Value 13.022 - Time Delta 14.036 - Time 2023-02-17T00:54:29\n",
            "Episode 320 - Step 62639 - Epsilon 0.9844622244194166 - Mean Reward 594.71 - Mean Length 169.92 - Mean Loss 0.667 - Mean Q Value 13.446 - Time Delta 21.143 - Time 2023-02-17T00:54:50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ca2279e340ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-f24acd30a51a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Get TD Target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtd_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Backpropagate loss through Q_online\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-78a62fd3a1fe>\u001b[0m in \u001b[0;36mtd_target\u001b[0;34m(self, reward, next_state, done)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtd_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnext_state_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"online\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_Q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     next_Q = self.net(next_state, model=\"target\")[\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3e59c4ff88aa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, model)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"online\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.35 GiB already allocated; 3.88 MiB free; 13.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}