{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP9q8U4jMAB/e4w/psI1Gv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viddev1/rl_ddqn_mario/blob/main/torch_rl_mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6nTX3XySTjE",
        "outputId": "aa172d61-583c-455a-885d-354c808e928b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 KB 4.1 MB/s eta 0:00:00\n",
            "Collecting nes-py>=8.1.4\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 KB 6.2 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.21.6)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 25.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.12.1)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py): started\n",
            "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp38-cp38-linux_x86_64.whl size=495619 sha256=36eb2de4febc113b1f0e95d21ee3019a2e7db8cce9b0f6f03a409822923811ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/e5/5c/8dfae61b44dbf56c458483aa09accef55a650e0527f6cbd872\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, collections\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "import gym_super_mario_bros\n"
      ],
      "metadata": {
        "id": "KSx_MIH0Sf_X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if gym.__version__ < '0.26':\n",
        "  env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "  env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = 'rgb', \n",
        "                                  apply_api_compatibility=True)\n",
        "\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape}, \\n {reward}, \\n {done},\\n {info}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAbRdkjUTPW1",
        "outputId": "cd44733e-df0b-4d98-81f8-dc93af60ff57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3), \n",
            " 0.0, \n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "  def __init__(self, env, skip):\n",
        "    super().__init__(env)\n",
        "    self.skip = skip\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0.0\n",
        "    for i in range(self._skip):\n",
        "      obs, reward, done, trunc, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return obs, total_reward, done, trunc, info\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    obs_shape = self.observation_space.shape[:2]\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def permute_orientation(self, observation):\n",
        "    observation = np.transpose(observation, (2, 0, 1))\n",
        "    observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "    return observation\n",
        "\n",
        "  def observation(self, observation):\n",
        "    observation = self.permute_orientation(observation)\n",
        "    transform = T.Grayscale()\n",
        "    observation = transform(observation)\n",
        "    return observation\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env, shape):\n",
        "    super().__init__(env)\n",
        "    if isinstance(shape, int):\n",
        "      self.shape = (shape, shape)\n",
        "    else:\n",
        "      self.shape = tuple(shape)\n",
        "    \n",
        "    obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def observation(self, observation):\n",
        "    transforms = T.Compose(\n",
        "        [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "    )\n",
        "    observation = transforms(observation).squeeze(0)\n",
        "    return observation\n",
        "\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "  env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "  env = FrameStack(env, num_stack=4)"
      ],
      "metadata": {
        "id": "wWwDKCT-UgWm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "  def __init__():\n",
        "    pass\n",
        "  \n",
        "  def act(self, state):\n",
        "    \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
        "    pass\n",
        "\n",
        "  def cache(self, experience):\n",
        "    \"\"\"Add the experience to memory\"\"\"\n",
        "    pass\n",
        "\n",
        "  def recall(self):\n",
        "    \"\"\"Sample experiences from memory\"\"\"\n",
        "    pass\n",
        "\n",
        "  def learn(self):\n",
        "    \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "4K5JEyMBZeSV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.save_dir = save_dir\n",
        "\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "    self.net = self.net.to(device=self.device)\n",
        "\n",
        "    self.exploration_rate = 1\n",
        "    self.exploration_rate_decay = 0.99999975\n",
        "    self.exploration_rate_min = 0.1\n",
        "    self.curr_step = 0\n",
        "\n",
        "    self.save_every = 5e5\n",
        "\n",
        "  def act(self, state):\n",
        "    \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    action_idx (int): An integer representing which action Mario will perform\n",
        "    \"\"\"\n",
        "\n",
        "    #Explore\n",
        "    if np.random.rand() < self.exploration_rate:\n",
        "      action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "    #Exploit\n",
        "    else:\n",
        "      state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "      state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "      action_values = self.net(state, model=\"online\")\n",
        "      action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "    #Decrease exploration_rate\n",
        "    self.exploration_rate *= self.exploration_eate_decay\n",
        "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "    #increment step\n",
        "    self.curr_step += 1\n",
        "    return action_idx\n",
        "\n"
      ],
      "metadata": {
        "id": "nqfhT3mFaw6y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.memory = deque(maxlen=100000)\n",
        "    self.batch_size = 32\n",
        "\n",
        "  def cache(self, state, next_state, action, reward, done):\n",
        "    \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (LazyFrame),\n",
        "        next_state (LazyFrame),\n",
        "        action (int),\n",
        "        reward (float),\n",
        "        done(bool))\n",
        "        \"\"\"\n",
        "    def first_if_tuple(x):\n",
        "      return x[0] if isinstance(x, tuple) else x\n",
        "    state = first_if_tuple(state).__array__()\n",
        "    next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "    state = torch.tensor(state, device= self.device)\n",
        "    next_state = torch.tensor(next_state, device=self.device)\n",
        "    action = torch.tensor([action], device=self.device)\n",
        "    reward = torch.tensor([reward], device=self.device)\n",
        "    done = torch.tensor([done], device=self.device)\n",
        "\n",
        "    self.memory.append((state, next_state, action, reward, done,))\n",
        "  \n",
        "  def recall(self):\n",
        "    \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "\n",
        "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "    "
      ],
      "metadata": {
        "id": "3fPp0O4adoJJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarioNet(nn.Module):\n",
        "  \"\"\"mini cnn structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    c, h, w = input_dim\n",
        "\n",
        "    if h != 84:\n",
        "      raise ValueError(f\"Expenting input height: 84, got: {h}\")\n",
        "    if w != 84:\n",
        "      raise ValueError(f\"Expenting input width: 84, got: {w}\")\n",
        "\n",
        "    self.online = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, output_dim)\n",
        "    )\n",
        "\n",
        "    self.target = copy.deepcopy(self.online)\n",
        "\n",
        "    # Q_target parameters are frozen.\n",
        "    for p in self.target.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "  def forward(self, input, model):\n",
        "    if model == \"online\":\n",
        "      return self.online(input)\n",
        "    elif model == \"target\":\n",
        "      return self.target(input)"
      ],
      "metadata": {
        "id": "z2oS1W2Ff4-5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.gamma = 0.9\n",
        "  \n",
        "  def td_estimate(self, state, action):\n",
        "    current_Q = self.net(state, model=\"online\")[\n",
        "        np.arrange(0, self.batch_size), action\n",
        "    ]\n",
        "    return current_Q\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def td_target(self, reward, next_state, done):\n",
        "    next_state_Q = self.net(next_state, model=\"online\")\n",
        "    best_action = torch.argmax(next_state_Q, axis=1)\n",
        "    next_Q = self.net(next_state, model=\"target\")[\n",
        "        np.arrange(0, self.batch_size), best_action\n",
        "    ]\n",
        "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ],
      "metadata": {
        "id": "5haEV_tGjaTz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "  def update_Q_online(self, td_estimate, td_target):\n",
        "    loss = self.loss_fn(td_estimate, td_target)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "  def sync_Q_target(self):\n",
        "    self.net.target.load_state_dict(self.net.online.state_dict())"
      ],
      "metadata": {
        "id": "gPhRSTDGlTDz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def save(self):\n",
        "    save_path = (\n",
        "        self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "    )\n",
        "    torch.save(\n",
        "        dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "        save_path\n",
        "    )\n",
        "    print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ],
      "metadata": {
        "id": "kuQXrt5jmb-S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.burnin = 1e4 # min. experiences before training\n",
        "    self.learn_every = 3 # no. of experiences between updates to Q_online\n",
        "    self.sync_every = 1e4 # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "  def learn(self):\n",
        "    if self.curr_step % self.sync_every == 0:\n",
        "      self.sync_Q_target()\n",
        "    \n",
        "    if self.curr_step % self.save_every == 0:\n",
        "      self.save()\n",
        "\n",
        "    if self.curr_step < self.burnin:\n",
        "      return None, None\n",
        "    \n",
        "    if self.curr_step % self.learn_every !=0:\n",
        "      return None, None\n",
        "    \n",
        "    # Sample from memory\n",
        "    state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "    # Get TD Estimate\n",
        "    td_est = self.td_estimate(state, action)\n",
        "\n",
        "    # Get TD Target\n",
        "    td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "    # Backpropagate loss through Q_online\n",
        "    loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "    return (td_est.mean().item(), loss)"
      ],
      "metadata": {
        "id": "Mejqtah0nERT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2e2HJDg-oVAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}